## 实验1.2 中文文本分类

### 1. 实验流程

1. 算法和流程

   ![文本聚类](/home/lantian/File/AI/photo/文本聚类.png)

   * 首先实现分词算法

     * `jieba`
     * `FMM`
     * `BMM`

   * 构建词袋

     对词向量的文本词频统计

   * `TF-IDF`

     1. `TF` :

        * 词频

        * 目的 : 计算词条在文本中出现的概率大小

        * 公式

          $$TF=\frac{N_{token}}{N}$$

     2. `IDF`

        * 逆文件词频

        * 目的 : 判断医德单词是否足够代表一个文本

        * 公式

          $$IDF=log(\frac{N + 1}{N_{exist} + 1})$$

     3. `TF-IDF`

        $$TF\_IDF=TF\times IDF$$

   * `PCA`降维操作

   * 聚类

2. 依赖包

   * `jieba` : 中文文本分词库
   * `sklearn` : 
     1. 生成向量空间模型
     2. 并创建`TF-IDF`矩阵
     3. `PCA`降维操作
     4. `KMeans`聚类操作

3. 语料库

   1. 复旦大学计算机信息与技术系国际数据库中心自然语言处理小组
   2. 共`9804`篇文档，分为`20`个类别

4. 遇到的困难和解决思路

   1. 语料库的编码和整理问题

      * 在实验的过程中，我发现语料库的编码问题非常的混乱，我自己动手写了2个脚本对异常编码文件进行了过了和恢复

      * 统一转码成`UTF-8`格式

      * 脚本示例

        1. 异常编码转换

           ```python
           #!/usr/bin/python3
           # 该文件用来将编码统一转换成utf8

           import os
           import glob

           ans = glob.glob('./*')
           print(ans)

           error_count = 0

           for i in ans:
               a = os.system('enca -L zh_CN -x utf-8 %s/*' % i)
               if a != 0 : error_count += 1

           print(error_count , len(ans))
           ```


        2. 错误编码删除

           ```python
           #!/usr/bin/python3
           # 该模块用来清理语料库中的异常编码文件
           import os
           import glob

           ans = glob.glob('./*')

           delete = []

           for i in ans:
               inner = glob.glob('%s/*' % i)
               dn = []
               for j in inner:
                   try:
                       f = open(j, 'r')
                       a = f.read()
                       f.close()
                   except:
                       dn.append(j)
                       f.close()
               delete.append(dn)
               print(len(dn), len(inner))
           ```

   2. 中文停用词解析

      * 不像英文被研究的过分的深入，中文的停用词研究一直比较不成熟，我在这里为了保证最后的聚类结果的精确性，决定删除词袋中的停用词，保证特征的`充分和完备性`

      * 停用词表

        1. `UTF-8`编码

        2. 数目 : `1893`个停用词

        3. 在`python`中使用集合对所有的停用词筛选

        4. 源代码 (59 ~ 66行是利用集合对停用词的筛选过滤)

           ```python
           #!/usr/bin/python3
           # Author : GMFTBY, shaw, sunny
           # Time   : 2017.12.29

           '''
           1. 语料库架加载模块
           2. 加载语料库中的数据
           3. 分词，引入停用词(jieba / 自定义模块)
           '''

           import os
           import jieba    # 这里可以改成我们的模块,仅做测试使用
           import glob
           import os
           import random

           def read_kind(kind = 'NULL', count=0):
               '''
                   1. 根据种类kind读取语料库中count个数据文件
                   2. 返回文档的列表，每一个元素是一个文档字符串
               '''
               ans = glob.glob('../data/train/%s/*' % kind)
               length = len(ans)
               result = []
               if length == 0 : 
                   print('请输入正确的分类')
                   return
               else:
                   if count > length : count = length
                   print('分类正确，返回 %d 个文档' % count)
                   # 打乱顺序，保证随机性
                   random.shuffle(ans)
               i = 0
               index = 0
               while i < count :
                   try:
                       if index >= count :
                           print('可用编码文件数目不足')
                           return 
                       with open(ans[i], 'r') as f:
                           result.append(f.read())
                           i += 1
                           index += 1
                   except:
                       print('一个文件编码出错')
                       index += 1
               return result

           def read_stopwords():
               '''
                   读取中文停用词后返回停用词列表
               '''
               import os
               os.system('pwd')
               with open('../data/train/stopwords', 'r') as f:
                   content = f.read()
               return content.split()

           def cut_without_stopwords(text, stopwords):
               '''
                   返回没有停用词的切词列表，一次输入一个文本的内容
               '''
               ans = set(jieba.lcut(text, HMM = True))
               stopwords = set(stopwords)
               # 集合操作去除停用词
               return list(ans - (ans & stopwords))

           if __name__ == "__main__":
               ans = read_kind('C19-Computer', 5)
               stopwords = read_stopwords()
               for i in ans:
                   print(cut_without_stopwords(i, stopwords))
           ```

   3. 计算的复杂性

      * 在实验的过程中我发现，如果向量空间中的向量的维度非常大的话，生成的`TF-IDF`矩阵也会变的非常的庞大并且矩阵还是稀疏的

      * 为了降低之后的计算的复杂度，我采用了`PCA`降维的思路将`TF-IDF`矩阵的维度降低到固定的维度数目(`100 / 200`)

        之后的计算的速度就会变得非常的迅速

      * 实验中使用`sklearn`算法库中的`PCA`实例实现

        ```python
        #!/usr/bin/python3
        # Author : GMFTBY
        # Time   : 2017.12.29

        '''
        PCA 降维模块
        '''

        from sklearn.decomposition import PCA

        def create_PCA(TF_IDF, weight):
            pca = PCA(n_components = weight)
            X = pca.fit_transform(TF_IDF)
            return X
        ```

      * 运算结果分析

        1. `PCA`降维前

           ![运算结果1](/home/lantian/File/AI/photo/运算结果1.png)

        2. `PCA`降维后

           ![运算结果2](/home/lantian/File/AI/photo/运算结果2.png)

        3. 结果分析

           1. 平均正确率有所下降

              因为`PCA`的计算本来就是存在有偏差的，难免会出现正确率下降的情况，但是基本可以容忍

           2. 运算时间大幅缩小

              因为引入了`PCA`降维操作，对于`TF-DF`矩阵的规模做了大幅度的调整，运算速度明显上升

### 2. 实验结果分析

1. 平均正确率

   5个`100`规模的分类文档的语料库下的运算结果

   ![运算结果1](/home/lantian/File/AI/photo/运算结果1.png)

2. 实验结果分析

   1. 从实验的平均正确率上我们可以看到，文本聚类算法`KMeans`的效果其实并不是很显著，但是已尽可以成功的将绝大多数的文本正确的分类

   2. 当前的算法上的一些补充的要点和不足之处

      * 可以考虑采用其他的聚类算法检验效果的是否有所提升

        * `BIRCH`
        * `DBSCAN`

      * 向量空间模型过于庞大，因为我在试验中是使用将所有的经过停用词筛选的分词结果的`并集`当做是一个词袋的向量空间

        当语料库的规模非常的庞大的时候，向量空间模型将会变的非常的庞大，需要考虑对向量控件模型进行缩减

        我已经考虑到的想法

        1. 对常用的名词做筛选，对于一些不常用的或者说没有分类意义的词筛去 : 需要额外的语料库的支持
        2. 非法的词条过滤，分词的结果并不一定全部都是合法的，对于非法的词条特征仔细的筛选过滤
        3. 出现次数过滤法 : 
           * 有些词出现的过于频繁，在每一个文档中出现次数基本一致(**计算思路可以使用`方差`实现**) : 可以考虑筛去
           * 有些词出现的从来没有出现过，计算的时候会导致`TF-IDF`矩阵过于稀疏，可以考虑筛去(**但是需要注意不要筛去特征词，慎重考虑**)

### 3. 源代码

1. `main`

   ```python
   #!/usr/bin/python3
   # Author : GMFTBY, shaw, sunny
   # Time   : 2017.12.29

   '''
   模块简介:
       1. 目的 : 对文本进行聚类分析(对于文本的标签判定可以作为之后的补充内容)
       2. 算法 : 聚类 + PCA降维 + TF-IDF + VSM
       3. 语料库 : 复旦大学计算机信息与技术系国际数据库中心自然语言处理小组
       4. 环境 :
           * Python3.6
           * jieba
           * sklearn
           * matplotlib
           * numpy
   '''

   # 模块文件
   # __all__  = ['loadcorpus', 'clustering', 'VSM', 'TFIDF', 'PCA', 'visualization']
   # __name__ = 'textscanner'

   # 测试代码块
   import loadcorpus as tl
   import clustering as tc
   import visualization as tv
   import VSM as tV
   import PCA as tP
   import TFIDF as tT
   import time

   def test():
       kind_list = ['C3-Art', 'C19-Computer', 'C7-History', 'C32-Agriculture', 'C31-Enviornment']
       stopwords = tl.read_stopwords()
       doc = []
       for i in kind_list:
           ans = tl.read_kind(i, 50)
           for j in ans:
               doc.append(tl.cut_without_stopwords(j, stopwords))
       print('分词完成')
       pdoc = []
       for i in doc:
           con = ' '.join(i)
           pdoc.append(con)
       from sklearn.feature_extraction.text import TfidfTransformer
       from sklearn.feature_extraction.text import CountVectorizer
       vectorizer = CountVectorizer()    
       transformer = TfidfTransformer()
       tfidf = transformer.fit_transform(vectorizer.fit_transform(pdoc))
       word = vectorizer.get_feature_names() #所有文本的关键字
       weight = tfidf.toarray()              #对应的tfidf矩阵
       '''
       ans, bag = tV.create_VSM(doc)
       print('词袋模型构建完毕')
       TF_IDF = tT.cal_TFIDF(ans)
       print('TF-IDF权重矩阵计算完毕')
       # TF_IDF = tP.create_PCA(TF_IDF, int(0.1 * TF_IDF.shape[1]))
       '''
       cluster = tc.KMeans(weight, len(kind_list))
       print('获得分类结果')
       return cluster, kind_list

   if __name__ == "__main__":
       begin = time.time()
       ans, kind = test()
       end = time.time()
       # 计算平均正确率
       length = len(kind)
       from collections import Counter
       sump = 0
       for i in range(length):
           pause = ans[i * 50 : i * 50 + 49]
           w = Counter(pause)
           top = w.most_common(1)[0][1] / 50
           sump += top
       print('平均正确率 :', sump / length)
       print('运算时间(s):', end - begin)

   ```

   ​