## 实验1.1 垃圾邮件分类

### 1. 实验流程

1. 算法和问题

   采用朴素是贝叶斯算法实现**标称型二分类问题**

2. 依赖包

   * `jieba` : 中文分词库
   * `glob` : 系统目录库
   * `numpy` : 科学计算库
   * `pickle` : python对象序列化工具

3. 遇到的注意事项记录

   1. 未登录词的影响，在词袋模型中进行忽略
   2. 邮件前缀中加入了消除前缀操作，抽取出邮件的文本内容
   3. 对于标点符号，非`ASCII`等特殊字符加入过滤器中统一过滤处理，尽可能保留中文文本的特征
   4. 对于训练的计算复杂性过高，采用模型的本地保存策略，加快模型加载速度
   5. 计算结果下溢
   6. 词汇特征表中存在大量的不存在的特征的判断

4. 语料库

   1. 网址链接 : `https://plg.uwaterloo.ca/~gvcormac/treccorpus06/`
   2. 名称 : **2006 TREC Public Spam Corpora**  (2006TREC公用垃圾邮件语料库)
   3. 语料库大小 : 
      * **64620**个中文邮件
      * 实验的我从中随机的抽取了一些邮件作为数据集

### 2. 实验结果分析

1. 精确率，召回率

   在`500 spam`,`500 ham`的训练集下，对`100`个随机数据集进行分析得到结果

   * 精确率 : `98.87%`
   * 召回率 : `69.72 % `

   在`100 spam`,`100 ham`的训练集下，对`100`个随机数据集进行分析得到的结果

   * 精确率 : `97.42%`
   * 召回率 : `65.44%`

2. 实验结果分析

   1. 对于朴素贝叶斯算法来说，虽然在少数据量下依然可以工作的很好，但是实际上数据量的提升对于我们的分类结果有显著的提升

   2. 对于监督式的算法，可能在算法思路上的一些不足，导致评判的结果并不是非常的优秀

      个人总结出来一下可以优化的要点

      * 尽量压缩特征的数目，对于一片邮件来说，并不需要大量的特征，几个关键的特征就足以说明一份邮件的性质
      * 特征的优先级，对于某些词一旦出现就有很高的概率判定这个邮件的性质，但是朴素贝叶斯的思想中对每一个特征的评价都是同等重要的，这个特性没有办法体现出来
      * 多次允运行选择最优结果，采用**交叉验证**的方式可以对当前算法的性能进行全面的分析

   3. 对于词袋模型和词集模型的思考

      * 词袋模型

        1. 在抽取的邮件的特征中，我们对特征的出现进行**计数**，采用数据记录特征的出现次数
        2. 目的是为了强调某一个特征的出现次数对于邮件的分类性质的判断

      * 词集模型

        1. 在抽取的邮件的特征中，我们对特征的出现进行**记录**，采用布尔型变量记录特征是否出现
        2. 目的是为了强调朴素的计算结果，强调每一个特征的都是同等重要的

      * 结果 :

        对于两者我都进行了实验，这里记录简要的实验结果

        1. 大数据量下，词袋模型的结果相对要优于词集模型的计算结果
        2. 但是词袋模型的计算开销相对较大

### 3. 实验克服的困难

1. 未登录词

   * 从训练集中抽取的特征并不一定会涵盖所有的邮件特征
   * 采用的策略 : 忽略未登录词的影响
   * 优点
     * 简化代码结构
     * 特征是固定的，有利于本地的序列化保存
   * 缺点
     * 可能会忽略未登录词对于邮件的判断性质的影响
     * 如果一份邮件中出现大量的未登录词，对于邮件的判断将会变得困难

2. 邮件前缀

   * 语料库的邮件中都存在有大量的前缀
   * 对于邮件前缀的忽略和正文抽取的想法
     1. 正则表达式 : 
        * 需要导入`re`包
        * 但是邮件的属性不相同，抽取正文结果比较苦难
        * 方便
        * 特殊字符和标点符号的处理不完善
     2. 自动构建停用词列表
        * 将遇见过的标点符号和所有的前缀词加入停用词列表
        * 遍历并排除所有的不符条件的行，只留下正文

3. 计算结果下溢

   1. 朴素贝叶斯算法需要计算条件概率

      $$P(w|c) = P(x_1,x_2,...,x_n|c)=P(x_1|c)P(x_2|c)...P(x_n|c)$$

   2. 但是条件概率中每一个乘数的值都非常小，结果导致 $$P(w|c)$$ 的结果也会非常小，出现数值下溢

   3. 采用 $$log$$ 对 $$P(w|c)$$ 进行处理，因为对条件概率进行对数运算并不改变数值的极大值点，并不会对结果有任何的影响

### 4. 源代码

1. 语料库抽取脚本

   ```python
   #!/usr/bin/python3

   # extract the data , 500 - spam, 500 - ham , 100 - test_data

   import os
   import glob
   import random

   with open('index', 'r') as f:
       item = f.read()
       items = item.split('\n')

   # 训练集
   spam_count = 0    # 垃圾邮件的个数
   ham_count  = 0    # 正常邮件的个数
   spam_list = []
   ham_list  = []
   index_list = []

   # 测试集
   test_count = 0
   test_list  = []
   test_label = []

   # 抽取训练集
   for i in items:
       if i.strip() == '': continue
       label, path = i.split()
       for_count = 0
       if label == 'spam' : for_count = spam_count
       else : for_count = ham_count

       if ham_count == 500 and spam_count == 500 : break
       if label == 'ham' and ham_count == 500 : continue
       if label == 'spam' and spam_count == 500 : continue

       ans = os.system('iconv -f GBK -t UTF-8 %s > ../new/train/%s/%s' \
               % (path, label, str(for_count)))
       if ans != 0:
           # 转换出错，结果不可信
           os.system('\rm ../new/train/%s/%s' % (label, str(for_count)))
       else:
           if label == 'spam' : 
               spam_count += 1
               spam_list.append('../new/train/%s/%s' % (label, str(for_count)))
           else : 
               ham_count += 1
               ham_list.append('../new/train/%s/%s' % (label, str(for_count)))
       index_list.append(path)

   # 抽取测试集
   length = len(items)
   times  = 0
   while True:
       index = random.randint(0, length - 1)
       label, i = items[index].split()
       if i in index_list : continue
       else:
           ans = os.system('iconv -f GBK -t UTF-8 %s > ../new/test/%s' % (i, str(times)))
           if ans != 0:
               os.system('\rm ../new/test/%s' % str(times))
           else:
               times += 1
               test_list.append(i)
               test_label.append(label)
           if times == 100 : break

   # 将训练集标签写入文件
   with open('test_label', 'w') as f:
       for i in range(len(test_list)):
           f.write(test_list[i] + ' ' + test_label[i] + '\n')
   ```

2. 朴素贝叶斯分类器

   ```python
   #!/usr/bin/python3
   # Author : GMFTBY
   # Time   : 2017.12.26

   '''
   本文件是对中文的垃圾邮件分类系统，使用朴素贝叶斯算法作为分类器
   对于中文的分词使用jieba等包进行相关处理

   1. 忽略未登录词造成的影响
   2. 邮件前缀加入消除操作
   3. 标点符号过滤，特殊符号过滤
   '''

   import jieba
   import glob
   import numpy as np
   import pickle

   def loaddataset():
       filte = [',', '.', ' ', '\n', '\t', '?', '!', '(', ')', '{', '}', '[', ']', '/', '-', '=', '+', '%', '#', '@', '~', '$', '^', '&', '*', '_', ':', ';', '\'', '"', '！', '。', '，', '（', '）', '：', '？', '￥', '＠', '＆', '‘', '“', '；', '｛', '｝', '－' ,'＋', '＝', '─', '》', '《', '☆', '、', '…', '\\', '|', '━', '／', 'ノ', '⌒', '〃', '⌒', 'ヽ', '”', '◇', '\u3000', '～', '╭', '╮', '〒', '失敗', '〒', '╭', '╮', 'Ω', '≡', 'ξ', '◤', 'μ', 'Θ', '◥', '█', '敗', '◤', '▎', 'υ', 'Φ', '│', '╰', '╯', '【']
       postinglist = []
       en_zh_filter = 'abcdefghigklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＳＴＵＶＷＸＹＺ'
       # 数据集加载
       classvec = []
       # 加载训练集
       spam = glob.glob('data/new/train/spam/*')
       for ii in spam:
           with open(ii, 'r') as f:
               content = []
               meet = 0
               for i in f.readlines():
                   if 'X-Mailer' in i or 'Content-Type' in i or 'X-MimeOLE' in i or 'Errors-To' in i or 'Content-Transfer-Encoding' in i or 'Message-Id' in i or 'Subject' in i:
                       meet = 1
                       continue
                   elif meet != 0:
                       line = jieba.lcut(i, HMM=True)
                       for p in line:
                           if p not in filte and p.isnumeric() == False:
                               for k in p:
                                   if k in en_zh_filter:
                                       break
                               else:
                                   content.append(p)
               postinglist.append(content)
               classvec.append(1)
       ham = glob.glob('data/new/train/ham/*')
       for ii in ham:
           with open(ii, 'r') as f:
               content = []
               meet = 0
               for i in f.readlines():
                   if 'X-Mailer' in i or 'Content-Type' in i or 'X-MimeOLE' in i or 'Errors-To' in i or 'Content-Transfer-Encoding' in i or 'Message-Id' in i or 'Subject' in i:
                       meet = 1
                       continue
                   elif meet != 0:
                       line = jieba.lcut(i, HMM=True)
                       for p in line:
                           if p not in filte and p.isnumeric() == False:
                               for k in p:
                                   if k in en_zh_filter:
                                       break
                               else:
                                   content.append(p)
               postinglist.append(content)
               classvec.append(0)
       return postinglist, classvec

   def createvocablist(dataset):
       vocabset = set([])
       for i in dataset:
           vocabset = vocabset | set(i)
       return list(vocabset)

   def bagofwords2vec(vocablist, inputset):
       # 返回文档的词袋模型
       returnvec = [0] * len(vocablist)
       for word in inputset:
           if word in vocablist:
               returnvec[vocablist.index(word)] += 1
           else:
               print('[%s] 不在词汇特征表中,自动忽略未登录词词' % word)
       return returnvec

   def trainNB(trainndarray, traincategory):
       # 训练朴素贝叶斯分类器
       numtraindocs = len(trainndarray)
       numwords = len(trainndarray[0])
       pab = sum(traincategory) * 1.0 / numtraindocs   # P(c)
       p0num = np.zeros(numwords)
       p1num = np.zeros(numwords)
       p0denom = 0.0
       p1denom = 0.0
       for i in range(numtraindocs):
           # 广播运算
           if traincategory[i] == 1:
               p1num += trainndarray[i]
               p1denom += sum(trainndarray[i])
           else:
               p0num += trainndarray[i]
               p0denom += sum(trainndarray[i])
       p1vect = np.zeros(numwords)
       p0vect = np.zeros(numwords)
       for ind, i in enumerate(p1num):
           if i == 0 :
               p1vect[ind] = 0
           else:
               p1vect[ind] = i * 1.0 / p1denom
       for ind, i in enumerate(p0num):
           if i == 0:
               p0vect[ind] = 0
           else:
               p0vect[ind] = i * 1.0 /p0denom
       return p0vect, p1vect, pab

   def classify(vec2classify, p0vect, p1vect, pclass1):
       # 朴素贝叶斯分类器
       p1 = sum(vec2classify * p1vect) + np.log(pclass1)
       p0 = sum(vec2classify * p0vect) + np.log(1 - pclass1)
       if p1 > p0:
           # 分类为垃圾邮件
           return 1
       else:
           # 分类为正常邮件
           return 0

   def train_error_count():
       # 检测训练集错误率,正确率
       data, label = loaddataset()
       vocablist = createvocablist(data)

       sum_count   = len(data)
       error_count = 0
       right_count = 0
       zhaohui_count = 0

       trainndarray = []
       for ind, i in enumerate(data):
           trainndarray.append(bagofwords2vec(vocablist, i))
           print('训练集加载 :', ind / sum_count, end = '\r')
       p0v, p1v, pab = trainNB(np.array(trainndarray), np.array(label))
       for ind, i in enumerate(data):
           thisdoc = np.array(bagofwords2vec(vocablist, i))
           ans = classify(thisdoc, p0v, p1v, pab)
           if ans == 0 and label[ind] == 0 : right_count += 1
           if ans == 1 and label[ind] == 1 : zhaohui_count += 1
           print('训练集测试 :', ind / sum_count, end='\r')
       return right_count * 1.0 / (len(data) / 2), zhaohui_count * 1.0 / (len(data) / 2)

   def loaddatatest():
       # 加载测试集
       filte = [',', '.', ' ', '\n', '\t', '?', '!', '(', ')', '{', '}', '[', ']', '/', '-', '=', '+', '%', '#', '@', '~', '$', '^', '&', '*', '_', ':', ';', '\'', '"', '！', '。', '，', '（', '）', '：', '？', '￥', '＠', '＆', '‘', '“', '；', '｛', '｝', '－' ,'＋', '＝', '─', '》', '《', '☆', '、', '…', '\\', '|', '━', '／', 'ノ', '⌒', '〃', '⌒', 'ヽ', '”', '◇', '\u3000', '～', '╭', '╮', '〒', '失敗', '〒', '╭', '╮', 'Ω', '≡', 'ξ', '◤', 'μ', 'Θ', '◥', '█', '敗', '◤', '▎', 'υ', 'Φ', '│', '╰', '╯', '【']
       postinglist = []
       en_zh_filter = 'abcdefghigklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＳＴＵＶＷＸＹＺ'
       tests = glob.glob('data/new/test/*')
       tests.sort()
       for ii in tests:
           with open(ii, 'r') as f:
               content = []
               meet = 0
               for i in f.readlines():
                   if 'X-Mailer' in i or 'Content-Type' in i or 'X-MimeOLE' in i or 'Errors-To' in i or 'Content-Transfer-Encoding' in i or 'Message-Id' in i or 'Subject' in i:
                       meet = 1
                       continue
                   elif meet != 0:
                       line = jieba.lcut(i, HMM=True)
                       for p in line:
                           if p not in filte and p.isnumeric() == False:
                               for k in p:
                                   if k in en_zh_filter:
                                       break
                               else:
                                   content.append(p)
               postinglist.append(content)
       label = []
       with open('test_label', 'r') as f:
           for i in f.readlines():
               if 'spam' in i : label.append(1)
               else : label.append(0)
       return postinglist, label

   def test_error_count():
       # 检测测试集错误率
       data, label = loaddataset()
       vocablist = createvocablist(data)
       testdata, testlabel = loaddatatest()
       sum_count   = len(testdata)
       error_count = 0
       right_count = 0
       zhaohui_count = 0

       trainndarray = []
       lenp = len(data)
       for ind, i in enumerate(data):
           trainndarray.append(bagofwords2vec(vocablist, i))
           print('测试集加载 :', ind / lenp, end = '\r')
       p0v, p1v, pab = trainNB(np.array(trainndarray), np.array(label))
       for ind, i in enumerate(testdata):
           thisdoc = np.array(bagofwords2vec(vocablist, i))
           ans = classify(thisdoc, p0v, p1v, pab)
           if ans == 0 and testlabel[ind] == 0 : right_count += 1
           if ans == 1 and testlabel[ind] == 1 : zhaohui_count += 1
           print('测试集测试 :', ind / sum_count, end= '\r')
           if ind == 10 : break
       return right_count * 1.0 / (sum_count / 2), zhaohui_count * 1.0 / (sum_count / 2)

   def save_model(p0v, p1v, pab):
       # 模型序列化
       with open('model', 'w') as f:
           f.write(str(p0v) + '\n')
           f.write(str(p1v) + '\n')
           f.write(str(pab) + '\n')

   def load_model():
       with open('model', 'r') as f:
           p0v = float(f.readline())
           p1v = float(f.readline())
           pab = float(f.readline())
       print(p0v, p1v, pab)
       return p0v, p1v, pab

   if __name__ == "__main__":
       '''
       data, label = loaddataset()
       vocablist = createvocablist(data)
       a = ['我','真','的','是','非常','抱歉','啊','我']
       trainndarray = []
       length = len(data)
       for ind, i in enumerate(data):
           trainndarray.append(bagofwords2vec(vocablist, i))
           print('训练集加载 :', ind / length, end='\r')
       p0v, p1v, pab = trainNB(np.array(trainndarray), np.array(label))
       thisdoc = np.array(bagofwords2vec(vocablist, a))
       ans = classify(thisdoc, p0v, p1v, pab)
       if ans == 0: ans = '正常邮件'
       else : ans = '垃圾邮件'
       print('朴素贝叶斯分类器归类邮件为 :', ans)
       '''
       print()
       ans2, ans3 = train_error_count()
       print('训练集正确率，召回率 :', ans2, ans3)
   ```

   ​

