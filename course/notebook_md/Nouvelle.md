## Nouvelle AI

1. 当前的只能是没有表现和理解能力的**智能**，并不是完备的智能
2. 纲要
   * Autonomous Agents : 自动的个体
   * Reinforcement Learning : 强化学习

### 自动个体

1. 什么是自动的个体 : 接收外界响应并做出反应的个体

2. 特点

   * 环境 : 基于环境的个体，并不是脱离实际现实的一种匿想
   * 自动 : 无需人为干预
   * 主动 : 有目标性的行为
   * 响应 : 可以对外界的环境作出响应
   * 交互 : 可以和其他的个体交互

3. 强智能

   除了上述的特点之外，还需要

   * 信仰
   * 欲望
   * 意图
   * 知识等等

4. `BDI`模型

   * 信息 : 知识，假设，信念
   * 动机 : 欲望，目标
   * 计划 : 意图

### 强化学习

1. 什么是学习

   * 个体和世界之间的相互作用
   * 个体应该不仅可以行动，而且可以提高之后的个体的表现的更好的能力(在未来获取最优解的时候)

2. 学习的方式

   * 监督学习 : 学习的函数可以提供或者被给定
   * 无监督学习 : 没有任何关于输出的信息
   * 强化学习 : 可以从环境中接收对于自己的行为的评价

3. `RL` 的强化学习模型

   基于`试错交互`的从成功和失败中获得奖励和惩罚的学习方法

   * 环境给个体提供当前的**状态**和**奖励或者惩罚**

   * 个体给环境提供下一步的动作

   * 定义`RL`问题

     1. 用`马尔科夫模型`定义

     2. 奖励的计算方式

        * 累加(无界函数)
        * 平均
        * 折扣累加(有界函数，可以优化)

     3. 状态定义

        * $$V^{\pi}(S)$$ : 从 $$S$$ 状态开始执行 $$\pi$$ 后的奖惩
        * $$Q^{\pi}(s,a)$$ : 从 $$s$$ 状态开始执行 $$a$$ 过程之后的奖惩
        * $$T(s,a,s')$$ : 状态 $$s$$ 经过动作 $$a $$ 后办成状态 $$s'$$ 的变换
        * $$R(s)$$ : 立即价值收益，状态 $$s$$ 的立即接收的价值(不是延期的)
        * $$\gamma$$ : 折扣
        * $$\pi$$ : 行动策略
        * $$V^*(s) = max(R(s,a) + \gamma \sum T(s,a,s')V^*(s'))$$
          * 求出使得上式最大的下一步动作 $$a$$
          * 类似于动态规划

     4. 最优策略搜索算法

        * 值迭代算法

          ![值迭代算法](/home/lantian/File/AI/photo/值迭代算法.png)

        * 策略迭代算法

          ![策略迭代算法](/home/lantian/File/AI/photo/策略迭代算法.png)

     5. 算法

        1. 全盘都计算
        2. 基于上一次的棋盘状态计算
        3. 撞墙自动的回到原地计算

     6. ​